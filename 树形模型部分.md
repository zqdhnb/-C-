## 模型部分

在处理完16个数据文件后，得到$16*N_i*L$的数据集。

**特征工程初步不做考虑；**

首先模型使用xgr或者lgb，将数据集按照个体随机划分为8+8的训练集和测试集，训练集再根据8:2划分为训练数据和验证数据，做五折交叉验证。同时利用验证数据将模型做早停（输入模型参数即可）。

### **方案一**（0.82-0.88 不稳定 跟不同文件的数据分布有关）

将8个训练集合并为一个大的训练集：
$$
D_{train} = \sum_{i}^{N}{D_{i}^{train}},N=8
$$
验证集和测试集随之合并：
$$
D_{val} = \sum_{i}^{N}{D_{i}^{val}},N=8\\
D_{test} = \sum_{i}^{N}{D_{i}^{test}},N=8
$$
同时定义模型$XGR$或者$LGB$，通过训练集和验证集训练：
$$
XGR(D_{train},D_{val}) \quad or \quad LGB(D_{train},D_{val})
$$
最后进行测试：
$$
Result = XGB(D_{test})\quad or \quad LGB(D_{test})
$$

### **方案二**

训练集和验证集不做合并，仅合并测试集：
$$
D_{test} = \sum_{i}^{N}{D_{i}^{test}},N=8
$$
对于8个训练集及对应的验证集，我们建立一个模型列表，包含八个模型：
$$
List_{xgb} = \{XGB_1,XGB_2,\cdots,XGB_8\} \\
List_{lgb} = \{LGB_1,LGB_2,\cdots,LGB_8\}
$$
每一个模型用不同的训练集和验证集训练：
$$
XGR_i(D_{i}^{train},D_i^{val}) \quad or \quad LGB_i(D_i^{train},D_i^{val})\\
i=\{1,2,\cdots,8\}
$$
然后我们将模型通过加权求和，得到最终的大模型：
$$
XGB = \sum_{i}^{N}{\alpha_i * XGB_i}\\
LGB = \sum_{i}^{N}{\alpha_i * LGB_i}
$$
其中$\alpha$的取值初步用样本个数占比来确定(后续可能要把不同模型的效果作为参考值加入)
$$
\alpha_i=\frac{D_{i}^{train}+D_{i}^{val}}{\sum_{i}^{N}{(D_{i}^{train}+D_{i}^{val}})},N=8
$$
最后计算整个测试集的结果：
$$
Result = XGB(D_{test})\quad or \quad LGB(D_{test})
$$
**方案二等价于投票机制。**

**stacking暂时不加入，先看看投票机制的结果。**

**另外两种加权方式：**

（实验证明，并不是数据越多，训练得越好；015只有不到五百个数据，验证集有96,007有两千多数据，验证集准确率只有13）

根据验证集的平均准确率做加权

**根据最后一次验证集的准确率做加权**（效果很好）







### **方案三**

**Light Stacking**

模型集成主要考虑将不同模型的优点集合起来，提升整体的准确率。

这里我们采用XGB和LGB做浅集成，原理如下：

对于一个样本，以$y=7.8$为界限，可以分别用XGB和LGB得到两个不同的结果：
$$
y_{XGB} \quad and \quad y_{LGB}
$$
我们给模型结果附加一个置信度$\alpha$，用来表示模型输出结果的可信程度，用距离分类界限的距离来衡量：
$$
\alpha_{XGB} = \frac{|y_{XGB}-y|}{y}\\
\alpha_{LGB} = \frac{|y_{LGB}-y|}{y}
$$
我们选取置信程度更大的模型的结果作为最终结果：
$$
y_{pre}=argmax_{\{\alpha_{XGB},\alpha_{LGB}\}}\{y_{XGB},y_{LGB}\}
$$

### **方案四**

**（本方案来自19年阿里天池的真实案例）**

**Deep Stacking**

![image-20230724164852079](C:\Users\18733\AppData\Roaming\Typora\typora-user-images\image-20230724164852079.png)

我们使用三个模型得到三组预测值，然后将三组预测值投入LGB训练，最终得到测试结果。

